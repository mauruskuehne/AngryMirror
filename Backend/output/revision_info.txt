arguments: align_dataset_mtcnn.py ./unaligned_faces ./aligned_faces
--------------------
git hash: b'd85ad67db10dc3f3741582bafaf89d79e446a331'
--------------------
b'diff --git a/Make_classifier.py b/Make_classifier.py\nindex 4415811..9066fb0 100644\n--- a/Make_classifier.py\n+++ b/Make_classifier.py\n@@ -22,7 +22,7 @@ with tf.Graph().as_default():\n         print(\'Number of images: %d\' % len(paths))\n \n         print(\'Loading feature extraction model\')\n-        modeldir = \'./models/facenet/20190310-055158\'\n+        modeldir = \'./models/facenet/20180402-114759\'\n         facenet.load_model(modeldir)\n \n         images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\ndiff --git a/align/align_dataset_mtcnn.py b/align/align_dataset_mtcnn.py\nindex 2969612..21e08b0 100644\n--- a/align/align_dataset_mtcnn.py\n+++ b/align/align_dataset_mtcnn.py\n@@ -25,16 +25,19 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+\n from scipy import misc\n import sys\n import os\n import argparse\n import tensorflow as tf\n+#import tensorflow.compat.v1 as tf\n import numpy as np\n import facenet\n import detect_face\n import random\n from time import sleep\n+#tf.disable_v2_behavior()\n \n def main(args):\n     sleep(random.random())\n@@ -49,8 +52,8 @@ def main(args):\n     print(\'Creating networks and loading parameters\')\n     \n     with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n+        sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n         with sess.as_default():\n             pnet, rnet, onet = detect_face.create_mtcnn(sess, None)\n     \ndiff --git a/input/readme.md b/input/readme.md\ndeleted file mode 100644\nindex df16a5f..0000000\n--- a/input/readme.md\n+++ /dev/null\n@@ -1,13 +0,0 @@\n-\n-This is the path to put your classifier image before alignment.The structure may like this:\n-\n-* input\n-  * class1(for face recognition it\'s a person\'s name)\n-    * image1 (for face recognition it\'s a person\'s photo)\n-    * image2\n-    * ...\n-  * class2\n-    * image1\n-    * image2\n-    * ...\n-  * class...\ndiff --git a/realtime_facenet.py b/realtime_facenet.py\nindex 6ad89a7..6579ce8 100644\n--- a/realtime_facenet.py\n+++ b/realtime_facenet.py\n@@ -31,7 +31,7 @@ with tf.Graph().as_default():\n         # HumanNames = [\'Andrew\',\'Obama\',\'ZiLin\']    #train human name\n \n         print(\'Loading feature extraction model\')\n-        modeldir = \'./models/facenet/20190308-101738\'\n+        modeldir = \'./models/facenet/20191103-184032\'\n         facenet.load_model(modeldir)\n \n         images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\ndiff --git a/realtime_facenet_yolo.py b/realtime_facenet_yolo.py\nindex 52ab620..b736cd6 100644\n--- a/realtime_facenet_yolo.py\n+++ b/realtime_facenet_yolo.py\n@@ -17,10 +17,10 @@ import pickle\n from utils import *\n \n parser = argparse.ArgumentParser()\n-parser.add_argument(\'--model-cfg\', type=str, default=\'./yolo_cfg/yolov3-face.cfg\',\n+parser.add_argument(\'--model-cfg\', type=str, default=\'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/yolo_cfg/yolov3-face.cfg\',\n                     help=\'path to config file\')\n parser.add_argument(\'--model-weights\', type=str,\n-                    default=\'./yolo_weights/yolov3-wider_16000.weights\',\n+                    default=\'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/align/model-weights/yolov3-wider_16000.weights\',\n                     help=\'path to weights of model\')\n args = parser.parse_args()\n \n@@ -47,7 +47,7 @@ with tf.Graph().as_default():\n         input_image_size = 160\n \n         print(\'Loading feature extraction model\')\n-        modeldir = \'./models/facenet/20190308-101738\'\n+        modeldir = \'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/models/facenet/20180402-114759\'\n         facenet.load_model(modeldir)\n \n         images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n@@ -55,7 +55,7 @@ with tf.Graph().as_default():\n         phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n         embedding_size = embeddings.get_shape()[1]\n \n-        classifier_filename = \'./myclassifier/my_classifier.pkl\'\n+        classifier_filename = \'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/myclassifier/my_classifier.pkl\'\n         classifier_filename_exp = os.path.expanduser(classifier_filename)\n         with open(classifier_filename_exp, \'rb\') as infile:\n             (model, class_names) = pickle.load(infile)\ndiff --git a/realtime_facenet_yolo_gpu.py b/realtime_facenet_yolo_gpu.py\nindex 98fdba6..95b8aea 100644\n--- a/realtime_facenet_yolo_gpu.py\n+++ b/realtime_facenet_yolo_gpu.py\n@@ -19,7 +19,7 @@ from utils import *\n \n def get_args():\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\'--model\', type=str, default=\'yolo_weights/YOLO_Face.h5\',\n+    parser.add_argument(\'--model\', type=str, default=\'align/yolo_weights/YOLO_Face.h5\',\n                     help=\'path to model weights file\')\n     parser.add_argument(\'--anchors\', type=str, default=\'yolo_cfg/yolo_anchors.txt\',\n                     help=\'path to anchor definitions\')\n@@ -62,7 +62,7 @@ def _main():\n             input_image_size = 160\n \n             print(\'Loading feature extraction model\')\n-            modeldir = \'./models/facenet/20190310-055158\'\n+            modeldir = \'./models/facenet/20191123-134935\'\n             facenet.load_model(modeldir)\n \n             images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\ndiff --git a/realtime_facenet_yolo_tiny.py b/realtime_facenet_yolo_tiny.py\nindex 74a7f4f..a3041ee 100644\n--- a/realtime_facenet_yolo_tiny.py\n+++ b/realtime_facenet_yolo_tiny.py\n@@ -38,188 +38,160 @@ context = zmq.Context()\n socket = context.socket(zmq.PUB)\n socket.bind("tcp://*:5556")\n \n+minsize = 20  # minimum size of face\n+threshold = [0.6, 0.7, 0.7]  # three steps\'s threshold\n+factor = 0.709  # scale factor\n+margin = 44\n+frame_interval = 3\n+batch_size = 1000\n+image_size = 182\n+input_image_size = 160\n \n-with tf.Graph().as_default():\n-    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-    with sess.as_default():\n-        # pnet, rnet, onet = detect_face.create_mtcnn(sess, \'./models/\')\n-\n-        minsize = 20  # minimum size of face\n-        threshold = [0.6, 0.7, 0.7]  # three steps\'s threshold\n-        factor = 0.709  # scale factor\n-        margin = 44\n-        frame_interval = 3\n-        batch_size = 1000\n-        image_size = 182\n-        input_image_size = 160\n-\n-        print(\'Loading feature extraction model\')\n-        modelpath = "/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/model_mobile/angrymirror_big_facenet.tflite"\n-        \n-\n-        # Load TFLite model and allocate tensors.\n-        interpreter = tf.lite.Interpreter(model_path=modelpath)\n-        interpreter.allocate_tensors()\n-\n-        # Get input and output tensors.\n-        input_details = interpreter.get_input_details()\n-        output_details = interpreter.get_output_details()\n-        # Test model on random input data.\n-        images_placeholder = input_details[0][\'shape\']\n-\n-        embeddings = interpreter.get_tensor(output_details[0][\'index\'])\n-\n-        embedding_size = embeddings.shape[1]\n-\n-        classifier_filename = \'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/myclassifier/tiny_classifier.pkl\'\n-        classifier_filename_exp = os.path.expanduser(classifier_filename)\n-        with open(classifier_filename_exp, \'rb\') as infile:\n-            (model, class_names) = pickle.load(infile)\n-            print(\'load classifier file-> %s\' % classifier_filename_exp)\n-\n-        #video_capture = cv2.VideoCapture(\'/Users/samuelochsner/yoloface/outputs/_yoloface.avi\')\n-        video_capture = cv2.VideoCapture(0)\n-\n-        c = 0\n-\n-        print(\'Start Recognition!\')\n-        prevTime = 0\n-        while True:\n-            ret, frame = video_capture.read()\n-\n-            # frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)    #resize frame (optional)\n-\n-            curTime = time.time()    # calc fps\n-            timeF = frame_interval\n-\n-            if (c % timeF == 0):\n-                find_results = []\n-\n-                if frame.ndim == 2:\n-                    frame = facenet.to_rgb(frame)\n-                frame = frame[:, :, 0:3]\n-                #print(frame.shape[0])\n-                #print(frame.shape[1])\n-\n-                # Use YOLO to get bounding boxes\n-                blob = cv2.dnn.blobFromImage(frame, 1 / 255, (IMG_WIDTH, IMG_HEIGHT), [0, 0, 0], 1, crop=False)\n-\n-                # Sets the input to the network\n-                net.setInput(blob)\n-\n-                # Runs the forward pass to get output of the output layers\n-                outs = net.forward(get_outputs_names(net))\n-\n-                # Remove the bounding boxes with low confidence\n-                bounding_boxes = post_process(frame, outs, CONF_THRESHOLD, NMS_THRESHOLD)\n-                nrof_faces = len(bounding_boxes)\n-                ## Use MTCNN to get the bounding boxes\n-                # bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor)\n-                # nrof_faces = bounding_boxes.shape[0]\n-                #print(\'Detected_FaceNum: %d\' % nrof_faces)\n-\n-                if nrof_faces > 0:\n-                    # det = bounding_boxes[:, 0:4]\n-                    img_size = np.asarray(frame.shape)[0:2]\n-\n-                    # cropped = []\n-                    # scaled = []\n-                    # scaled_reshape = []\n-                    bb = np.zeros((nrof_faces,4), dtype=np.int32)\n-\n-                    for i in range(nrof_faces):\n-                        emb_array = np.zeros((1, embedding_size))\n-\n-                        bb[i][0] = bounding_boxes[i][0]\n-                        bb[i][1] = bounding_boxes[i][1]\n-                        bb[i][2] = bounding_boxes[i][2]\n-                        bb[i][3] = bounding_boxes[i][3]\n-\n-                        if bb[i][0] <= 0 or bb[i][1] <= 0 or bb[i][2] >= len(frame[0]) or bb[i][3] >= len(frame):\n-                            print(\'face is inner of range!\')\n-                            continue\n-\n-                        # cropped.append(frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\n-                        # cropped[0] = facenet.flip(cropped[0], False)\n-                        # scaled.append(misc.imresize(cropped[0], (image_size, image_size), interp=\'bilinear\'))\n-                        # scaled[0] = cv2.resize(scaled[0], (input_image_size,input_image_size),\n-                        #                        interpolation=cv2.INTER_CUBIC)\n-                        # scaled[0] = facenet.prewhiten(scaled[0])\n-                        # scaled_reshape.append(scaled[0].reshape(-1,input_image_size,input_image_size,3))\n-                        # feed_dict = {images_placeholder: scaled_reshape[0], phase_train_placeholder: False}\n-\n-                        cropped = (frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\n-                        print("{0} {1} {2} {3}".format(bb[i][0], bb[i][1], bb[i][2], bb[i][3]))\n-                        cropped = facenet.flip(cropped, False)\n-                        scaled = (misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\'))\n-                        scaled = cv2.resize(scaled, (input_image_size,input_image_size),\n-                                            interpolation=cv2.INTER_CUBIC)\n-                        scaled = facenet.prewhiten(scaled)\n-                        scaled_reshape = (scaled.reshape(-1,input_image_size,input_image_size,3))\n-\n-                        scaled_reshape = np.array(scaled_reshape, dtype=np.float32)\n-                        print(scaled_reshape.shape)\n-                        for i in range(0, scaled_reshape.shape[0]):\n-\n-                            imgs = scaled_reshape[i, :, :,:]\n-                            imgs = np.expand_dims(imgs, axis=0)\n-                            #imgs = tf.convert_to_tensor(imgs, np.uint8) \n-                            try:              \n-                                interpreter.set_tensor(input_details[0][\'index\'], imgs)\n-                                interpreter.invoke()\n-                                emb_array[i, :] = interpreter.get_tensor(output_details[0][\'index\'])\n-                                print("processed ", i)\n-                            except Exception:\n-                                print("Error with ", i)\n-                                pass\n-\n-                        predictions = model.predict_proba(emb_array)\n-\n-                        best_class_indices = np.argmax(predictions, axis=1)\n-                        best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n-                        print(best_class_probabilities)\n-                        cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n-                        text_x = bb[i][0]\n-                        text_y = bb[i][3] + 20\n-\n-                        # for H_i in HumanNames:\n-                        #     if HumanNames[best_class_indices[0]] == H_i:\n-                        result_names = class_names[best_class_indices[0]]\n-                        try:\n-                            socket.send(result_names.encode())\n-                        except Exception as ex:\n-                            print(ex)\n-                            pass\n-                        \n-                        #print(result_names)\n-                        cv2.putText(frame, result_names, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                    1, (0, 0, 255), thickness=1, lineType=2)\n-                else:\n-                    try:\n-                     \n-                            socket.send("nobody")\n-                    except Exception as ex:\n-                        print(ex)\n+print(\'Loading feature extraction model\')\n+modelpath = "/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/model_mobile/angrymirror_big_facenet.tflite"\n+\n+\n+# Load TFLite model and allocate tensors.\n+interpreter = tf.lite.Interpreter(model_path=modelpath)\n+interpreter.allocate_tensors()\n+\n+# Get input and output tensors.\n+input_details = interpreter.get_input_details()\n+output_details = interpreter.get_output_details()\n+# Test model on random input data.\n+images_placeholder = input_details[0][\'shape\']\n+\n+embeddings = interpreter.get_tensor(output_details[0][\'index\'])\n+\n+embedding_size = embeddings.shape[1]\n+\n+classifier_filename = \'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/myclassifier/tiny_classifier.pkl\'\n+classifier_filename_exp = os.path.expanduser(classifier_filename)\n+with open(classifier_filename_exp, \'rb\') as infile:\n+    (model, class_names) = pickle.load(infile)\n+    print(\'load classifier file-> %s\' % classifier_filename_exp)\n+\n+#video_capture = cv2.VideoCapture(\'/Users/samuelochsner/yoloface/outputs/_yoloface.avi\')\n+video_capture = cv2.VideoCapture(0)\n+\n+c = 0\n+\n+print(\'Start Recognition!\')\n+prevTime = 0\n+while True:\n+    ret, frame = video_capture.read()\n+\n+    # frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)    #resize frame (optional)\n+\n+    curTime = time.time()    # calc fps\n+    timeF = frame_interval\n+\n+    if (c % timeF == 0):\n+        find_results = []\n+\n+        if frame.ndim == 2:\n+            frame = facenet.to_rgb(frame)\n+        frame = frame[:, :, 0:3]\n+\n+        # Use YOLO to get bounding boxes\n+        blob = cv2.dnn.blobFromImage(frame, 1 / 255, (IMG_WIDTH, IMG_HEIGHT), [0, 0, 0], 1, crop=False)\n+\n+        # Sets the input to the network\n+        net.setInput(blob)\n+\n+        # Runs the forward pass to get output of the output layers\n+        outs = net.forward(get_outputs_names(net))\n+\n+        # Remove the bounding boxes with low confidence\n+        bounding_boxes = post_process(frame, outs, CONF_THRESHOLD, NMS_THRESHOLD)\n+        nrof_faces = len(bounding_boxes)\n+\n+        if nrof_faces > 0:\n+            img_size = np.asarray(frame.shape)[0:2]\n+\n+            bb = np.zeros((nrof_faces,4), dtype=np.int32)\n+\n+            for i in range(nrof_faces):\n+                emb_array = np.zeros((1, embedding_size))\n+\n+                bb[i][0] = bounding_boxes[i][0]\n+                bb[i][1] = bounding_boxes[i][1]\n+                bb[i][2] = bounding_boxes[i][2]\n+                bb[i][3] = bounding_boxes[i][3]\n+\n+                if bb[i][0] <= 0 or bb[i][1] <= 0 or bb[i][2] >= len(frame[0]) or bb[i][3] >= len(frame):\n+                    print(\'face is inner of range!\')\n+                    continue\n+\n+                cropped = (frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\n+                print("{0} {1} {2} {3}".format(bb[i][0], bb[i][1], bb[i][2], bb[i][3]))\n+                cropped = facenet.flip(cropped, False)\n+                scaled = (misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\'))\n+                scaled = cv2.resize(scaled, (input_image_size,input_image_size),\n+                                    interpolation=cv2.INTER_CUBIC)\n+                scaled = facenet.prewhiten(scaled)\n+                scaled_reshape = (scaled.reshape(-1,input_image_size,input_image_size,3))\n+\n+                scaled_reshape = np.array(scaled_reshape, dtype=np.float32)\n+                print(scaled_reshape.shape)\n+                for i in range(0, scaled_reshape.shape[0]):\n+\n+                    imgs = scaled_reshape[i, :, :,:]\n+                    imgs = np.expand_dims(imgs, axis=0)\n+\n+                    try:              \n+                        interpreter.set_tensor(input_details[0][\'index\'], imgs)\n+                        interpreter.invoke()\n+                        emb_array[i, :] = interpreter.get_tensor(output_details[0][\'index\'])\n+                        print("processed ", i)\n+                    except Exception:\n+                        print("Error with ", i)\n                         pass\n \n-                    print(\'Unable to align\')\n-\n-            sec = curTime - prevTime\n-            prevTime = curTime\n-            fps = 1 / (sec)\n-            str = \'FPS: %2.3f\' % fps\n-            text_fps_x = len(frame[0]) - 150\n-            text_fps_y = 20\n-            cv2.putText(frame, str, (text_fps_x, text_fps_y),\n-                        cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 0), thickness=1, lineType=2)\n-            # c+=1\n-            cv2.imshow(\'Video\', frame)\n-\n-            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n-                break\n-\n-        video_capture.release()\n-        # #video writer\n-        # out.release()\n-        cv2.destroyAllWindows()\n+                predictions = model.predict_proba(emb_array)\n+\n+                best_class_indices = np.argmax(predictions, axis=1)\n+                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n+                print(best_class_probabilities)\n+                cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n+                text_x = bb[i][0]\n+                text_y = bb[i][3] + 20\n+\n+\n+                result_names = class_names[best_class_indices[0]]\n+                try:\n+                    socket.send(result_names.encode())\n+                except Exception as ex:\n+                    print(ex)\n+                    pass\n+                \n+                #print(result_names)\n+                cv2.putText(frame, result_names, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n+                            1, (0, 0, 255), thickness=1, lineType=2)\n+        else:\n+            try:              \n+                socket.send(b"nobody")\n+            except Exception as ex:\n+                print(ex)\n+                pass\n+\n+            print(\'Unable to align\')\n+\n+    sec = curTime - prevTime\n+    prevTime = curTime\n+    fps = 1 / (sec)\n+    str = \'FPS: %2.3f\' % fps\n+    text_fps_x = len(frame[0]) - 150\n+    text_fps_y = 20\n+    cv2.putText(frame, str, (text_fps_x, text_fps_y),\n+                cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 0), thickness=1, lineType=2)\n+    # c+=1\n+    cv2.imshow(\'Video\', frame)\n+\n+    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n+        break\n+\n+video_capture.release()\n+# #video writer\n+# out.release()\n+cv2.destroyAllWindows()\ndiff --git a/train_tripletloss.py b/train_tripletloss.py\nindex 0f455c5..bc9e80d 100644\n--- a/train_tripletloss.py\n+++ b/train_tripletloss.py\n@@ -426,28 +426,28 @@ def parse_arguments(argv):\n     parser.add_argument(\'--logs_base_dir\', type=str, \n         help=\'Directory where to write event logs.\', default=\'./logs/facenet\')\n     parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'./models/facenet\')\n+        help=\'Directory where to write trained models and checkpoints.\', default=\'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/models/facenet\')\n     parser.add_argument(\'--gpu_memory_fraction\', type=float,\n         help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1)\n     parser.add_argument(\'--pretrained_model\', type=str,\n-        help=\'Load a pretrained model before training starts.\', default=\'./models/20170512-110547/model-20170512-110547.ckpt-250000\')\n+        help=\'Load a pretrained model before training starts.\', default=\'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/models/20170512-110547/model-20170512-110547.ckpt-250000\')\n     parser.add_argument(\'--data_dir\', type=str,\n         help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'./output\')\n+        default=\'/Users/samuelochsner/facerecognition/faceRecognition-yolo-facenet/output\')\n     parser.add_argument(\'--model_def\', type=str,\n         help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n     parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=50)\n+        help=\'Number of epochs to run.\', default=50) \n     parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=15)\n+        help=\'Number of images to process in a batch.\', default=12)\n     parser.add_argument(\'--image_size\', type=int,\n         help=\'Image size (height, width) in pixels.\', default=160)\n     parser.add_argument(\'--people_per_batch\', type=int,\n-        help=\'Number of people per batch.\', default=6)\n+        help=\'Number of people per batch.\', default=4)\n     parser.add_argument(\'--images_per_person\', type=int,\n-        help=\'Number of images per person.\', default=10)\n+        help=\'Number of images per person.\', default=30)\n     parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=250)\n+        help=\'Number of batches per epoch.\', default=5)\n     parser.add_argument(\'--alpha\', type=float,\n         help=\'Positive to negative triplet distance margin.\', default=0.3)\n     parser.add_argument(\'--embedding_size\', type=int,'